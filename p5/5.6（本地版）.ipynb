{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf1ed80",
   "metadata": {},
   "source": [
    "# 堪培拉天气预测：明天会下雨吗？\n",
    "\n",
    "本项目旨在使用澳大利亚堪培拉的历史天气数据，通过机器学习方法预测第二天的降雨情况。我们将会经历数据清洗、预处理、模型训练、评估和优化的完整流程。\n",
    "\n",
    "**最终模型**: XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14762708",
   "metadata": {},
   "source": [
    "## 1. 导入所需库\n",
    "首先，我们导入项目所需的全部 Python 库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e392801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6913e752",
   "metadata": {},
   "source": [
    "## 2. 加载与探索数据\n",
    "加载训练集、测试集和提交示例文件，并进行初步的数据探索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cab0dea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "train_df = pd.read_csv('./5.6_datasets/train_weather.csv', index_col='Unnamed: 0')\n",
    "test_df = pd.read_csv('./5.6_datasets/test_weather.csv', index_col='Unnamed: 0')\n",
    "submit_df = pd.read_csv('./5.6_datasets/submit_result.csv', index_col='Unnamed: 0')\n",
    "\n",
    "# 快速查看数据信息，包括数据类型和非空值数量\n",
    "print(\"--- 训练集信息 ---\")\n",
    "train_df.info()\n",
    "\n",
    "print(\"\\n--- 测试集信息 ---\")\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebdada7",
   "metadata": {},
   "source": [
    "## 3. 数据预处理\n",
    "数据预处理是机器学习流程中的关键步骤，包括处理异常值、缺失值和对特征进行编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad975f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤 3.1: 处理异常值\n",
    "# 根据数据描述，'Cloud9am' 和 'Cloud3pm' 的取值范围应为 [0, 8]。\n",
    "# 我们需要找出并删除超出此范围的异常数据行。\n",
    "train_df = train_df[train_df['Cloud3pm'] != 9]\n",
    "anomalous_test_indices = test_df[test_df['Cloud9am'] == 9].index\n",
    "test_df = test_df.drop(anomalous_test_indices)\n",
    "submit_df = submit_df.drop(anomalous_test_indices) # 提交文件也需同步删除\n",
    "\n",
    "# 步骤 3.2: 将 'Date' 列设为索引\n",
    "train_df.set_index('Date', inplace=True)\n",
    "test_df.set_index('Date', inplace=True)\n",
    "\n",
    "# 步骤 3.3: 填充缺失值 (NaN)\n",
    "# 我们将特征分为数值型和分类型，并采用不同的策略进行填充。\n",
    "\n",
    "# 识别分类型和数值型特征列\n",
    "categorical_cols = train_df.select_dtypes(include='object').columns.tolist()\n",
    "numerical_cols = train_df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "# 'RainTomorrow' 是目标变量，从特征列表中移除\n",
    "categorical_cols.remove('RainTomorrow')\n",
    "\n",
    "# 使用众数填充分类型特征的缺失值\n",
    "imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
    "train_df[categorical_cols] = imputer_categorical.fit_transform(train_df[categorical_cols])\n",
    "test_df[categorical_cols] = imputer_categorical.transform(test_df[categorical_cols])\n",
    "\n",
    "# 使用中位数填充数值型特征的缺失值（中位数对异常值更稳健）\n",
    "imputer_numerical = SimpleImputer(strategy='median')\n",
    "train_df[numerical_cols] = imputer_numerical.fit_transform(train_df[numerical_cols])\n",
    "test_df[numerical_cols] = imputer_numerical.transform(test_df[numerical_cols])\n",
    "\n",
    "# 步骤 3.4: 特征编码\n",
    "# 将所有分类型特征（包括目标变量）转换为数值，以便模型处理。\n",
    "all_categorical_cols = categorical_cols + ['RainTomorrow']\n",
    "for col in all_categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    # 对训练集进行拟合和转换\n",
    "    train_df[col] = le.fit_transform(train_df[col])\n",
    "    # 对测试集只进行转换（如果该列存在于测试集中）\n",
    "    if col in test_df.columns:\n",
    "        # 处理测试集中可能出现训练集未见过的新类别\n",
    "        test_df[col] = test_df[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
    "        le.classes_ = np.append(le.classes_, '<unknown>')\n",
    "        test_df[col] = le.transform(test_df[col])\n",
    "\n",
    "print(\"数据预处理完成。检查是否还有缺失值：\")\n",
    "print(train_df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c86708",
   "metadata": {},
   "source": [
    "## 4. 模型训练与评估\n",
    "在这一步，我们将数据划分为训练集和验证集，然后使用 XGBoost 模型进行训练和初步评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277aaa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤 4.1: 划分特征和目标变量\n",
    "X = train_df.drop('RainTomorrow', axis=1)\n",
    "y = train_df['RainTomorrow']\n",
    "\n",
    "# 步骤 4.2: 切分训练集和验证集\n",
    "# 采用 80/20 的比例划分，这是常见的做法\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 步骤 4.3: 构建并训练 XGBoost 模型\n",
    "model = XGBClassifier(\n",
    "    n_estimators=200,          # 树的数量\n",
    "    max_depth=5,               # 树的最大深度\n",
    "    learning_rate=0.1,         # 学习率\n",
    "    use_label_encoder=False,   # 已手动完成编码，关闭自动编码\n",
    "    eval_metric='logloss',     # 评估指标\n",
    "    n_jobs=-1,                 # 使用所有CPU核心\n",
    "    random_state=42            # 随机种子，保证结果可复现\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 步骤 4.4: 在验证集上进行预测和评估\n",
    "y_pred_val = model.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred_val)\n",
    "\n",
    "print(f\"在验证集上的初步准确率: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770ada47",
   "metadata": {},
   "source": [
    "## 5. 模型优化 (可选)\n",
    "使用 `GridSearchCV` 自动搜索最佳的超参数组合，以进一步提升模型性能。**注意：此过程在heywhale的2核8Gcpu上跑大概耗时6min。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f041ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义要搜索的参数网格\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.05, 0.1]\n",
    "}\n",
    "\n",
    "# 创建一个新的、未训练的模型实例用于网格搜索\n",
    "xgb_for_grid = XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 配置并运行 GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_for_grid, \n",
    "    param_grid=param_grid, \n",
    "    cv=3,                      # 3折交叉验证\n",
    "    scoring='accuracy', \n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# 在训练数据上执行搜索\n",
    "#grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 打印最佳参数和对应的分数\n",
    "print(f\"最佳参数: {grid_search.best_params_}\")\n",
    "print(f\"交叉验证最高准确率: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "print(\"网格搜索部分已注释，以节省运行时间。可取消注释以运行。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c144f4c",
   "metadata": {},
   "source": [
    "## 6. 生成最终提交文件\n",
    "使用训练好的模型对官方测试集进行预测，并生成 `submit.csv` 文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156481df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用我们训练好的初始模型 'model' 对测试集进行预测\n",
    "# 如果运行了网格搜索，也可以使用 grid_search.best_estimator_ 进行预测，效果可能更好\n",
    "final_predictions = model.predict(test_df)\n",
    "\n",
    "# 将预测结果填充到提交数据框中\n",
    "submit_df['RainTomorrow'] = final_predictions.astype(int)\n",
    "\n",
    "# 保存为 CSV 文件\n",
    "submit_df.to_csv('submission.csv')\n",
    "\n",
    "print(\"提交文件 'submission.csv' 已成功生成。\")\n",
    "print(submit_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
